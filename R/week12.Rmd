---
title: "week12"
author: "Lina Huang"
date: "2023-04-21"
output: html_document
---

```{r}
# Script Settings and Resources
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(httr)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(wordcloud)
library(tidytext)
library(topicmodels)
```

```{r}
#Data Import and Cleaning
# specify the URL and parameters for the Reddit API endpoint
url <- "https://www.reddit.com/r/IOPsychology/new.json"
params <- list(limit = 1000, t = "year")

# send a GET request to the API endpoint
response <- GET(url, query = params)

# extract the upvotes and titles of the posts from the JSON response
posts <- content(response)$data$children
upvotes <- sapply(posts, function(x) x$data$ups)
titles <- sapply(posts, function(x) x$data$title)

# create the week12_tbl tibble with upvotes and title variables
week12_tbl <- tibble(upvotes = upvotes, title = titles)

# save downloaded data to the data folder
write_csv(week12_tbl, "../data/week12_tbl.csv")
# Read data from the data folder
week12_tbl<-read_csv("../data/week12_tbl.csv", show_col_types = FALSE)
```


# Portion 7 Write a function called compare_them() that takes two corpora as input and displays one randomly selected row of content from each.Then use this function to display the same row from both corpora you just created.
```{r}
#Create io_corpus_original from the title column
io_corpus_original <- Corpus(VectorSource(week12_tbl$title))
# Function to remove specific words
remove_custom_words <- function(text, words_to_remove) {
  pattern <- paste0("\\b(", paste(words_to_remove, collapse = "|"), ")\\b")
  gsub(pattern, "", text, ignore.case = TRUE)
}
# Words to remove
words_to_remove <- c("io", "i/o", "i/o psychology", "io psychology", "iop", "psychology")

# Pre-processing and creating the io_corpus
io_corpus <- io_corpus_original %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(removeWords, words_to_remove) %>%
  tm_map(stripWhitespace) %>%
  tm_map(PlainTextDocument) %>%
  tm_map(content_transformer(lemmatize_strings))

#Create the compare_them() function
compare_them <- function(corpus1, corpus2) {
  # Get the number of documents in the corpora (assuming both have the same length)
  n_docs <- length(corpus1)
  
  # Generate a random index
  random_index <- sample(n_docs, 1)
  
  # Print the content of the randomly selected row for each corpus
  cat("Corpus 1 (Original): \n")
  writeLines(as.character(corpus1[[random_index]]))
  
  cat("\nCorpus 2 (Preprocessed): \n")
  writeLines(as.character(corpus2[[random_index]]))
}
# Run the function 10 times to display the same row from both corpora
for (i in 1:10) {
  cat("Comparison", i, "\n")
  compare_them(io_corpus_original, io_corpus)
  cat("\n\n")
}
```

# Portion8.Create a bigram DTM called io_dtm. Also create a version of this DTM with sparse terms eliminated called io_slim_dtm. Retain between a 2:1 and 3:1 N/k ratio in the slim DTM.
```{r}
#Create bigram DTM
io_dtm <- DocumentTermMatrix(io_corpus, control = list(wordLengths = c(2, 2)))

# Calculate threshold for minimum number of documents per term
k <- ncol(io_dtm)
N <- nrow(io_dtm)
threshold <- ceiling(N / (3 * k / 2))

# Create slim DTM with sparse terms removed
io_slim_dtm <- removeSparseTerms(io_dtm, sparse = 1 - threshold / N)

```

# Portion 9. Using good practices in topic modeling (and remember to explain everything clearly in comments), use latent Dirichlet allocation to categorize posts into topics from io_dtm. Create a tibble topics_tbl.
```{r}
# Portion 9.1
# Set the seed for reproducibility
set.seed(42)

# Choose the number of topics
k <- 5

# Perform LDA on the io_dtm
lda_model <- LDA(io_dtm, k = k, control = list(seed = 42))
# Extract the topic-document matrix (tdm) from the LDA model
tdm <- posterior(lda_model)$topics

# Create a tibble topics_tbl with the required columns
topics_tbl <- tibble(
  doc_id = 1:nrow(tdm),
  original = sapply(io_corpus, as.character),
  topic = apply(tdm, 1, which.max),
  probability = apply(tdm, 1, max)
)
topics_tbl #print tibble

# Portion 9.2 Respond to the following questions in comments: 
# Function to get top terms for a topic
get_top_terms <- function(topic_num, beta_matrix, terms, top_n) {
  term_probabilities <- beta_matrix[, topic_num]
  top_term_indices <- order(term_probabilities, decreasing = TRUE)[1:top_n]
  top_terms <- terms[top_term_indices]
  paste(top_terms, collapse = ", ")
}
# Display top_n terms for each topic
for (i in 1:k) {
  top_terms <- get_top_terms(i, beta_matrix, lda_model@terms, top_n)
  cat(paste("Topic", i, ":", top_terms), "\n")
}
#Using the beta matrix alone, what topics would you conclude your final topic list maps onto? 
#Since each topic has similar term, so my final topic list could map onto any of these topics.As the topics generated by the LDA model have the same terms, it is difficult to derive meaningful topics or substantive constructs from them. This indicates that the model has not been trained well or there is an issue with the dataset or preprocessing steps.


# Display the original text of documents with the highest and lowest probabilities for each topic
for (i in 1:k) {
  cat("Topic", i, "\n")
  
  highest_prob_doc <- topics_tbl %>%
    filter(topic == i) %>%
    top_n(1, wt = probability) %>%
    pull(original)
  cat("Highest probability document:", highest_prob_doc, "\n")
  
  lowest_prob_doc <- topics_tbl %>%
    filter(topic == i) %>%
    top_n(1, wt = -probability) %>%
    pull(original)
  cat("Lowest probability document:", lowest_prob_doc, "\n\n")
}

#Reviewing the original text of the documents with the highest and lowest probabilities for each topic can help us assess whether the topic names derived from the beta matrix conceptually match the content of the original posts. If they match well, it provides evidence of content validity, indicating that the topics identified by the LDA model represent meaningful and interpretable constructs in the dataset. It is hard to tell whether my topic names derived from my interpretation of the beta matrix conceptually match with the content of the original posts since my all 5 topics are having the same terms. 

```

